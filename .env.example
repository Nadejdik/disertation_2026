# LLM Evaluation Configuration

# API Keys (set these as environment variables)
OPENAI_API_KEY=your-api-key-here
LLAMA_API_ENDPOINT=http://localhost:8000/generate  # If using API
PHI3_API_ENDPOINT=http://localhost:8001/generate   # If using API

# Model Paths (for local inference)
LLAMA3_MODEL_PATH=path/to/llama-3-8b
PHI3_MODEL_PATH=microsoft/Phi-3-mini-4k-instruct

# Evaluation Configuration
N_SAMPLES=100          # Number of samples to evaluate per dataset
N_RUNS=3               # Number of runs per sample for consistency
TEMPERATURE=0.7        # Model temperature (0.0 = deterministic, 1.0 = creative)
MAX_TOKENS=2048        # Maximum tokens for model responses

# Rate Limiting
REQUESTS_PER_MINUTE=60
DELAY_BETWEEN_REQUESTS=0.5  # seconds

# Output Paths
RESULTS_DIR=results
DATASETS_DIR=datasets
MODELS_CACHE_DIR=models_cache

# Dataset Configuration
TELECOM_FAULTS_SAMPLES=1000
KPI_ANOMALY_SAMPLES=5000
MICROSERVICES_TRACES=1000
ANOMALY_RATIO=0.05

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/evaluation.log
